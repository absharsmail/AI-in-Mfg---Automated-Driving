{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#imports necessary to define a neural network \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#ensure you are using GPU.\n",
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:0\" \n",
    "else:  \n",
    "  dev = \"cpu\"  \n",
    "\n",
    "device = torch.device(dev)\n",
    "print(device)\n",
    "\n",
    "dtype = torch.cuda.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Ant Pc/.cache\\torch\\hub\\pytorch_vision_master\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision', 'alexnet', pretrained=False,force_reload=False)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=9, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "model.classifier[6]=nn.Linear(4096,9)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load('C:/Users/Ant Pc/GitHub/AI-in-Mfg---Automated-Driving/Aaftab/vijresnet152balance.pt'))\n",
    "model=model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "trdt1=[]\n",
    "trdt1=np.load('E:/v11/Copy of training_data-8.npy', allow_pickle=True)\n",
    "        \n",
    "for i in range(9,17):\n",
    "                file_name = 'E:/v11/Copy of training_data-{}.npy'.format(i)\n",
    "                # full file info\n",
    "                train_data = np.load(file_name,allow_pickle=True)\n",
    "                trdt1=np.concatenate((trdt1,train_data), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_X=[]\n",
    "val_Y=[]\n",
    "np.random.shuffle(trdt1)\n",
    "val_X=trdt1[:,0]\n",
    "val_Y=trdt1[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0306) tensor(0.5930, device='cuda:0', dtype=torch.float64) 0 335.80735206604004 89400\n",
      "tensor(0.0051, device='cuda:0') tensor(0.6196, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.0045) tensor(0.6669, device='cuda:0', dtype=torch.float64) 1 333.08970737457275 89400\n",
      "tensor(0.0049, device='cuda:0') tensor(0.6224, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.0041) tensor(0.6917, device='cuda:0', dtype=torch.float64) 2 331.09792971611023 89400\n",
      "tensor(0.0051, device='cuda:0') tensor(0.6111, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.0038) tensor(0.7138, device='cuda:0', dtype=torch.float64) 3 336.152583360672 89400\n",
      "tensor(0.0052, device='cuda:0') tensor(0.6151, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.0035) tensor(0.7350, device='cuda:0', dtype=torch.float64) 4 335.5342483520508 89400\n",
      "tensor(0.0055, device='cuda:0') tensor(0.6162, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.0032) tensor(0.7563, device='cuda:0', dtype=torch.float64) 5 341.49808049201965 89400\n",
      "tensor(0.0058, device='cuda:0') tensor(0.5989, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.0029) tensor(0.7784, device='cuda:0', dtype=torch.float64) 6 342.00345849990845 89400\n",
      "tensor(0.0061, device='cuda:0') tensor(0.6120, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.0026) tensor(0.8005, device='cuda:0', dtype=torch.float64) 7 342.10694885253906 89400\n",
      "tensor(0.0068, device='cuda:0') tensor(0.5873, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.0024) tensor(0.8232, device='cuda:0', dtype=torch.float64) 8 344.1673903465271 89400\n",
      "tensor(0.0073, device='cuda:0') tensor(0.5847, device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.utils import class_weight\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#trainingloop\n",
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.001\n",
    "batchsize=200\n",
    "\n",
    "epochs=300\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.Adagrad(model.parameters(), lr)\n",
    "\n",
    "for p in range(epochs):\n",
    "    k1=0\n",
    "    running_corrects = 0\n",
    "    t1=0\n",
    "    t=0\n",
    "    s=time.time()\n",
    "    for l in range(4):\n",
    "        trdt=[]\n",
    "        X=[]\n",
    "        Y=[]\n",
    "        if l==0:\n",
    "        \n",
    "            trdt=np.load('E:/v11/Copy of training_data-1.npy', allow_pickle=True)\n",
    "        \n",
    "            for i in range(2,7):\n",
    "                file_name = 'E:/v11/Copy of training_data-{}.npy'.format(i)\n",
    "                # full file info\n",
    "                train_data = np.load(file_name,allow_pickle=True)\n",
    "                trdt=np.concatenate((trdt,train_data), axis=0)\n",
    "            \n",
    "            for i in range(17,62):\n",
    "                file_name = 'E:/v11/Copy of training_data-{}.npy'.format(i)\n",
    "                # full file info\n",
    "                train_data = np.load(file_name,allow_pickle=True)\n",
    "                trdt=np.concatenate((trdt,train_data), axis=0)\n",
    "        \n",
    "        elif l==1:\n",
    "        \n",
    "            trdt=np.load('E:/v11/Copy of training_data-63.npy', allow_pickle=True)\n",
    "\n",
    "            for i in range(64,111):\n",
    "                file_name = 'E:/v11/Copy of training_data-{}.npy'.format(i)\n",
    "                # full file info\n",
    "                train_data = np.load(file_name,allow_pickle=True)\n",
    "                trdt=np.concatenate((trdt,train_data), axis=0)\n",
    "                \n",
    "        elif l==2:\n",
    "        \n",
    "            trdt=np.load('E:/v12/training_data-111.npy', allow_pickle=True)\n",
    "\n",
    "            for i in range(111,150):\n",
    "                file_name = 'E:/v12/training_data-{}.npy'.format(i)\n",
    "                # full file info\n",
    "                train_data = np.load(file_name,allow_pickle=True)\n",
    "                trdt=np.concatenate((trdt,train_data), axis=0)\n",
    "        \n",
    "        elif l==3:\n",
    "        \n",
    "            trdt=np.load('E:/v12/training_data-150.npy', allow_pickle=True)\n",
    "\n",
    "            for i in range(150,189):\n",
    "                file_name = 'E:/v12/training_data-{}.npy'.format(i)\n",
    "                # full file info\n",
    "                train_data = np.load(file_name,allow_pickle=True)\n",
    "                trdt=np.concatenate((trdt,train_data), axis=0)\n",
    "        \n",
    "        np.random.shuffle(trdt)\n",
    "        X=trdt[:,0]\n",
    "        Y=trdt[:,1]\n",
    "        \n",
    "        \n",
    "        batches=len(X)/batchsize\n",
    "        yo=[]\n",
    "        val_yo=[]\n",
    "        for i in range(len(X)):\n",
    "            yo.append(np.argmax(Y[i]))\n",
    "        \n",
    "        for val_i in range(len(val_X)):\n",
    "            val_yo.append(np.argmax(val_Y[val_i]))\n",
    "        \n",
    "        y=np.array(yo)\n",
    "        val_y=np.array(val_yo)\n",
    "        \n",
    "        for j in range(int(batches)):\n",
    "            k1=k1+batchsize\n",
    "            inp=[]\n",
    "            res=[]\n",
    "\n",
    "            for k in range(batchsize):\n",
    "                inp.append(cv2.resize(X[j*batchsize+k][30:-8,:],(166,240)))\n",
    "                res.append(y[j*batchsize+k])\n",
    "\n",
    "            inp=np.array(inp).reshape(batchsize,3,166,240)\n",
    "            inptens=torch.from_numpy(inp).type(dtype)\n",
    "\n",
    "            res=np.array(res)\n",
    "            restens=torch.from_numpy(res).type(dtype)\n",
    "\n",
    "            #forward pass\n",
    "            out=model(inptens).type(dtype)\n",
    "            #compute loss\n",
    "\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            loss = criterion(out,restens.long().squeeze()).type(dtype)\n",
    "\n",
    "            #backprop loss i.e. find dloss/dparam for each parameter and store.\n",
    "            loss.backward(retain_graph=False)\n",
    "            \n",
    "            #clip gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            #use optimiser to update\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, preds = torch.max(out, 1)\n",
    "\n",
    "            running_corrects += torch.sum(preds == restens)\n",
    "            t=loss\n",
    "            t1=t1+t.detach().cpu()\n",
    "            \n",
    "            del inptens\n",
    "            del restens\n",
    "            del t\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.reset_max_memory_allocated()\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    \n",
    "    val_running_corrects=0\n",
    "    val_loss=0\n",
    "    \n",
    "    val_inp=[]\n",
    "    val_res=[]\n",
    "    \n",
    "    for val_k in range(len(val_X)):\n",
    "                val_inp.append(cv2.resize(val_X[val_k][30:-8,:],(166,240)))\n",
    "                val_res.append(val_y[val_k])   \n",
    "                \n",
    "    b=batchsize\n",
    "    for t in range(int(len(val_X)/b)):\n",
    "        val_inp1=np.array(val_inp[t*b:t*b+b]).reshape(b,3,166,240)\n",
    "        val_inptens=torch.from_numpy(val_inp1).type(dtype)\n",
    "\n",
    "        val_res1=np.array(val_res[t*b:t*b+b])\n",
    "        val_restens=torch.from_numpy(val_res1).type(dtype)\n",
    "        \n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_out=model(val_inptens).type(dtype)\n",
    "        _, val_preds = torch.max(val_out, 1)\n",
    "\n",
    "        val_running_corrects += torch.sum(val_preds == val_restens)\n",
    "\n",
    "        val_criterion = nn.CrossEntropyLoss()\n",
    "        val_loss += criterion(val_out,val_restens.long().squeeze()).type(dtype)\n",
    "        \n",
    "        del val_inptens\n",
    "        del val_restens\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_max_memory_allocated()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    val_epoch_acc=val_running_corrects.double() /len(val_X)\n",
    "    \n",
    "    o=time.time()    \n",
    "    epoch_acc = running_corrects.double() /k1\n",
    "    print(t1/k1,epoch_acc,p,o-s,k1)\n",
    "    print(val_loss/len(val_X),val_epoch_acc)\n",
    "    \n",
    "    torch.save(model.state_dict(),'vijalexnet101balanceFINALpretrained.pt')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(X[j*batchsize+k])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[j*batchsize+k].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trdt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
