{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oracle Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "#imports necessary to define a neural network \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#ensure you are using GPU.\n",
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:0\" \n",
    "else:  \n",
    "  dev = \"cpu\"  \n",
    "\n",
    "device = torch.device(dev)\n",
    "print(device)\n",
    "\n",
    "dtype = torch.cuda.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 28395])\n",
      "torch.Size([30, 28395])\n",
      "torch.Size([200])\n",
      "torch.Size([200])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trdt1=pd.read_csv('C:\\\\Users\\\\Ant Pc\\\\GitHub\\\\pROJ\\\\Genotype\\\\TrainingGenotype.csv')\n",
    "trdt=pd.read_csv('C:\\\\Users\\\\Ant Pc\\\\GitHub\\\\pROJ\\\\Geneexpression0\\\\traingeneexpression.csv',header=None)\n",
    "tstdt=pd.read_csv('C:\\\\Users\\\\Ant Pc\\\\GitHub\\\\pROJ\\\\Geneexpression0\\\\testgeneexpression.csv',header=None)\n",
    "\n",
    "tstdt=np.array(tstdt)\n",
    "tstdt=np.transpose(tstdt)\n",
    "tstdt=pd.DataFrame(tstdt)\n",
    "\n",
    "trdt=np.array(trdt)\n",
    "trdt=np.transpose(trdt)\n",
    "trdt=pd.DataFrame(trdt)\n",
    "\n",
    "results1=trdt1['Phenotype1']\n",
    "results2=trdt1['Phenotype2']\n",
    "# convert matrices to pytorch tensors on gpu\n",
    "\n",
    "trdttensor=torch.from_numpy(trdt.values).type(dtype)\n",
    "tstdttensor=torch.from_numpy(tstdt.values).type(dtype)\n",
    "results1tensor=torch.from_numpy(results1.values).type(dtype)\n",
    "results2tensor=torch.from_numpy(results2.values).type(dtype)\n",
    "\n",
    "print(trdttensor.shape)\n",
    "print(tstdttensor.shape)\n",
    "print(results2tensor.shape)\n",
    "print(results1tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold=pd.read_csv('C:/Users/Ant Pc/GitHub/PROJECT/DATA/DREAM5_SysGenB2_GoldStandard.csv', sep='\\t')\n",
    "gold.drop(gold.columns[-1],axis=1,inplace=True)\n",
    "\n",
    "gold2=gold.iloc[1,:]\n",
    "gold2=gold2.values\n",
    "\n",
    "dt2=np.transpose(gold2)\n",
    "\n",
    "gold1=gold.iloc[0,:]\n",
    "gold1=gold1.values\n",
    "\n",
    "dt1=np.transpose(gold1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net1(\n",
      "  (rand): Linear(in_features=28395, out_features=1000, bias=True)\n",
      "  (fc1): Linear(in_features=1000, out_features=3200, bias=True)\n",
      "  (fc2): Linear(in_features=3200, out_features=3200, bias=True)\n",
      "  (fc3): Linear(in_features=3200, out_features=3200, bias=True)\n",
      "  (fc6): Linear(in_features=3200, out_features=1, bias=True)\n",
      ")\n",
      "tensor(12.7451, device='cuda:0', grad_fn=<L1LossBackward>) 0\n",
      "tensor(8.5119, device='cuda:0', grad_fn=<L1LossBackward>) 1\n",
      "tensor(7.2074, device='cuda:0', grad_fn=<L1LossBackward>) 2\n",
      "tensor(6.3478, device='cuda:0', grad_fn=<L1LossBackward>) 3\n",
      "tensor(5.7181, device='cuda:0', grad_fn=<L1LossBackward>) 4\n",
      "tensor(5.2680, device='cuda:0', grad_fn=<L1LossBackward>) 5\n",
      "tensor(4.8932, device='cuda:0', grad_fn=<L1LossBackward>) 6\n",
      "tensor(4.5714, device='cuda:0', grad_fn=<L1LossBackward>) 7\n",
      "tensor(4.3111, device='cuda:0', grad_fn=<L1LossBackward>) 8\n",
      "tensor(4.6031, device='cuda:0', grad_fn=<L1LossBackward>) 9\n",
      "tensor(4.1968, device='cuda:0', grad_fn=<L1LossBackward>) 10\n",
      "tensor(5.2054, device='cuda:0', grad_fn=<L1LossBackward>) 11\n",
      "tensor(4.6268, device='cuda:0', grad_fn=<L1LossBackward>) 12\n",
      "tensor(4.2731, device='cuda:0', grad_fn=<L1LossBackward>) 13\n",
      "tensor(4.2384, device='cuda:0', grad_fn=<L1LossBackward>) 14\n",
      "tensor(3.9989, device='cuda:0', grad_fn=<L1LossBackward>) 15\n",
      "tensor(2.5012, device='cuda:0', grad_fn=<L1LossBackward>) 16\n",
      "tensor(2.4420, device='cuda:0', grad_fn=<L1LossBackward>) 17\n",
      "tensor(2.4915, device='cuda:0', grad_fn=<L1LossBackward>) 18\n",
      "tensor(2.4675, device='cuda:0', grad_fn=<L1LossBackward>) 19\n",
      "tensor(2.5864, device='cuda:0', grad_fn=<L1LossBackward>) 20\n",
      "tensor(2.4728, device='cuda:0', grad_fn=<L1LossBackward>) 21\n",
      "tensor(2.5425, device='cuda:0', grad_fn=<L1LossBackward>) 22\n",
      "tensor(2.5026, device='cuda:0', grad_fn=<L1LossBackward>) 23\n",
      "tensor(2.5088, device='cuda:0', grad_fn=<L1LossBackward>) 24\n",
      "tensor(2.5283, device='cuda:0', grad_fn=<L1LossBackward>) 25\n",
      "tensor(2.5462, device='cuda:0', grad_fn=<L1LossBackward>) 26\n",
      "tensor(2.5625, device='cuda:0', grad_fn=<L1LossBackward>) 27\n",
      "tensor(2.5784, device='cuda:0', grad_fn=<L1LossBackward>) 28\n",
      "tensor(2.5912, device='cuda:0', grad_fn=<L1LossBackward>) 29\n",
      "tensor(2.5883, device='cuda:0', grad_fn=<L1LossBackward>) 30\n",
      "tensor(2.6021, device='cuda:0', grad_fn=<L1LossBackward>) 31\n",
      "tensor(2.6045, device='cuda:0', grad_fn=<L1LossBackward>) 32\n",
      "tensor(2.6071, device='cuda:0', grad_fn=<L1LossBackward>) 33\n",
      "tensor(2.6090, device='cuda:0', grad_fn=<L1LossBackward>) 34\n",
      "tensor(2.6118, device='cuda:0', grad_fn=<L1LossBackward>) 35\n",
      "tensor(2.6212, device='cuda:0', grad_fn=<L1LossBackward>) 36\n",
      "tensor(2.6281, device='cuda:0', grad_fn=<L1LossBackward>) 37\n",
      "tensor(2.6237, device='cuda:0', grad_fn=<L1LossBackward>) 38\n",
      "tensor(2.6220, device='cuda:0', grad_fn=<L1LossBackward>) 39\n",
      "tensor(2.6297, device='cuda:0', grad_fn=<L1LossBackward>) 40\n",
      "tensor(2.6257, device='cuda:0', grad_fn=<L1LossBackward>) 41\n",
      "tensor(2.6260, device='cuda:0', grad_fn=<L1LossBackward>) 42\n",
      "tensor(2.6225, device='cuda:0', grad_fn=<L1LossBackward>) 43\n",
      "tensor(2.6226, device='cuda:0', grad_fn=<L1LossBackward>) 44\n",
      "tensor(2.6209, device='cuda:0', grad_fn=<L1LossBackward>) 45\n",
      "tensor(2.6234, device='cuda:0', grad_fn=<L1LossBackward>) 46\n",
      "tensor(2.6191, device='cuda:0', grad_fn=<L1LossBackward>) 47\n",
      "tensor(2.6159, device='cuda:0', grad_fn=<L1LossBackward>) 48\n",
      "tensor(2.6191, device='cuda:0', grad_fn=<L1LossBackward>) 49\n",
      "tensor(2.6164, device='cuda:0', grad_fn=<L1LossBackward>) 50\n",
      "tensor(2.6169, device='cuda:0', grad_fn=<L1LossBackward>) 51\n",
      "tensor(2.6165, device='cuda:0', grad_fn=<L1LossBackward>) 52\n",
      "tensor(2.6152, device='cuda:0', grad_fn=<L1LossBackward>) 53\n",
      "tensor(2.6165, device='cuda:0', grad_fn=<L1LossBackward>) 54\n",
      "tensor(2.6157, device='cuda:0', grad_fn=<L1LossBackward>) 55\n",
      "tensor(2.6167, device='cuda:0', grad_fn=<L1LossBackward>) 56\n",
      "tensor(2.6182, device='cuda:0', grad_fn=<L1LossBackward>) 57\n",
      "tensor(2.6175, device='cuda:0', grad_fn=<L1LossBackward>) 58\n",
      "tensor(2.6168, device='cuda:0', grad_fn=<L1LossBackward>) 59\n",
      "tensor(2.6186, device='cuda:0', grad_fn=<L1LossBackward>) 60\n",
      "tensor(2.6212, device='cuda:0', grad_fn=<L1LossBackward>) 61\n",
      "tensor(2.6173, device='cuda:0', grad_fn=<L1LossBackward>) 62\n",
      "tensor(2.6205, device='cuda:0', grad_fn=<L1LossBackward>) 63\n",
      "tensor(2.6238, device='cuda:0', grad_fn=<L1LossBackward>) 64\n",
      "tensor(2.6233, device='cuda:0', grad_fn=<L1LossBackward>) 65\n",
      "tensor(2.6217, device='cuda:0', grad_fn=<L1LossBackward>) 66\n",
      "tensor(2.6277, device='cuda:0', grad_fn=<L1LossBackward>) 67\n",
      "tensor(2.6245, device='cuda:0', grad_fn=<L1LossBackward>) 68\n",
      "tensor(2.6278, device='cuda:0', grad_fn=<L1LossBackward>) 69\n",
      "tensor(2.6295, device='cuda:0', grad_fn=<L1LossBackward>) 70\n",
      "tensor(2.6320, device='cuda:0', grad_fn=<L1LossBackward>) 71\n",
      "tensor(2.6289, device='cuda:0', grad_fn=<L1LossBackward>) 72\n",
      "tensor(2.6364, device='cuda:0', grad_fn=<L1LossBackward>) 73\n",
      "tensor(2.6369, device='cuda:0', grad_fn=<L1LossBackward>) 74\n",
      "tensor(2.6405, device='cuda:0', grad_fn=<L1LossBackward>) 75\n",
      "tensor(2.6413, device='cuda:0', grad_fn=<L1LossBackward>) 76\n",
      "tensor(2.6409, device='cuda:0', grad_fn=<L1LossBackward>) 77\n",
      "tensor(2.6423, device='cuda:0', grad_fn=<L1LossBackward>) 78\n",
      "tensor(2.6500, device='cuda:0', grad_fn=<L1LossBackward>) 79\n"
     ]
    }
   ],
   "source": [
    "randmatr=pd.read_csv('C:/Users/Ant Pc/GitHub/PROJECT/Randmatr.csv', header=None).values\n",
    "class Net1(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net1, self).__init__()\n",
    "        \n",
    "        # an affine operation: y = Wx + b\n",
    "        \n",
    "        self.rand=nn.Linear(28395,1000).cuda()\n",
    "        self.fc1=nn.Linear(1000,3200).cuda()\n",
    "        self.fc2=nn.Linear(3200,3200).cuda()\n",
    "        self.fc3=nn.Linear(3200,3200).cuda()\n",
    "        self.fc6 = nn.Linear(3200,1).cuda()\n",
    "        \n",
    "        #set weights as random projection matrix\n",
    "        self.rand.weight.data = torch.transpose(torch.from_numpy(randmatr),0,1).type(dtype)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.rand(x).cuda()\n",
    "        x = F.relu(x).cuda()\n",
    "        x = F.relu(self.fc1(x)).cuda() \n",
    "        x = F.relu(self.fc2(x)).cuda()   \n",
    "        x = F.relu(self.fc3(x)).cuda()      \n",
    "        x = self.fc6(x).cuda()\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = Net1()\n",
    "\n",
    "#use gpu for all computations in model\n",
    "model.cuda()\n",
    "print(model)\n",
    "\n",
    "#trainingloop\n",
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.0001\n",
    "batchsize=20\n",
    "\n",
    "batches=len(trdttensor)/batchsize\n",
    "\n",
    "epochs=80\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.Adagrad(model.parameters(), lr)\n",
    "\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    for j in range(int(batches)):\n",
    "        \n",
    "        #forward pass\n",
    "        out=model(trdttensor[j:j+batchsize,:].type(dtype))\n",
    "\n",
    "        #compute loss\n",
    "        criterion = nn.MSELoss()\n",
    "        loss = criterion(torch.reshape(out,[20]).type(dtype),torch.reshape(results2tensor[j:j+batchsize],[20]).type(dtype)).type(dtype)\n",
    "\n",
    "\n",
    "        #backprop loss i.e. find dloss/dparam for each parameter and store.\n",
    "        loss.backward(retain_graph=True)\n",
    "        \n",
    "        #clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        #use optimiser to update\n",
    "        optimizer.step()\n",
    "    c=nn.L1Loss()\n",
    "    print(c(torch.reshape(model(tstdttensor),[30]),torch.from_numpy(np.array(dt2)).type(dtype)),i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[21.0765],\n",
       "        [26.4424],\n",
       "        [20.9737],\n",
       "        [21.9576],\n",
       "        [17.3590],\n",
       "        [21.9875],\n",
       "        [19.9204],\n",
       "        [19.2679],\n",
       "        [19.5645],\n",
       "        [24.7051],\n",
       "        [22.3846],\n",
       "        [19.9419],\n",
       "        [23.6470],\n",
       "        [35.4223],\n",
       "        [18.0090],\n",
       "        [19.9170],\n",
       "        [20.4232],\n",
       "        [18.9201],\n",
       "        [20.7503],\n",
       "        [18.5378]], device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(trdttensor[0:20,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OGGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2831, -0.0210,  0.5410,  0.1419, -0.8578,  0.9457],\n",
      "        [ 0.0375,  0.0723, -0.6105,  0.1935,  0.1172,  3.4136],\n",
      "        [-1.6872, -1.4617, -0.4798,  0.9317,  1.0890, -1.1767],\n",
      "        ...,\n",
      "        [ 0.6879, -0.9719,  1.3608,  0.6761, -2.2764,  0.5467],\n",
      "        [-1.5506, -0.3979, -1.5016, -1.1532,  1.2028,  1.3251],\n",
      "        [-0.6512,  0.1571,  0.5789,  0.6581, -1.5121,  2.9956]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#random input matrix\n",
    "nsamples=3000\n",
    "X=np.random.randn(nsamples,6)\n",
    "\n",
    "Xtensor=torch.from_numpy(X).type(dtype)\n",
    "print(Xtensor[:,0:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customloss(matr):\n",
    "    #input is tensor of size(1,28395)\n",
    "    \n",
    "    inpu=matr\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    pred=model(matr).type(dtype)\n",
    "    ideal=45*torch.ones_like(pred).type(dtype)\n",
    "    loss=criterion(pred,ideal).type(dtype)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (inp): Linear(in_features=6, out_features=3200, bias=True)\n",
      "  (fc1): Linear(in_features=3200, out_features=3200, bias=True)\n",
      "  (fc2): Linear(in_features=3200, out_features=28395, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Neural network to generate geneexpression values\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # an affine operation: y = Wx + b\n",
    "        self.inp =nn.Linear(6,3200).cuda()\n",
    "        self.fc1=nn.Linear(3200,3200).cuda()\n",
    "        self.fc2 = nn.Linear(3200,28395).cuda()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=F.relu(self.inp(x)).cuda()\n",
    "        x = self.fc1(x).cuda()\n",
    "        x = self.fc2(x).cuda()\n",
    "        x=torch.abs(x).cuda()\n",
    "        x[x>10]=x[x>10]/10\n",
    "        x[x<1]=x[x<1]*100\n",
    "        x=x.cuda()\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "net.cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(62.2916, device='cuda:0', grad_fn=<MseLossBackward>) 0\n",
      "tensor(28.7720, device='cuda:0', grad_fn=<MseLossBackward>) 1\n",
      "tensor(15.4282, device='cuda:0', grad_fn=<MseLossBackward>) 2\n",
      "tensor(9.5249, device='cuda:0', grad_fn=<MseLossBackward>) 3\n",
      "tensor(6.3478, device='cuda:0', grad_fn=<MseLossBackward>) 4\n",
      "tensor(4.9480, device='cuda:0', grad_fn=<MseLossBackward>) 5\n",
      "tensor(3.5314, device='cuda:0', grad_fn=<MseLossBackward>) 6\n",
      "tensor(3.1139, device='cuda:0', grad_fn=<MseLossBackward>) 7\n",
      "tensor(2.2698, device='cuda:0', grad_fn=<MseLossBackward>) 8\n",
      "tensor(2.1668, device='cuda:0', grad_fn=<MseLossBackward>) 9\n",
      "tensor(2.2770, device='cuda:0', grad_fn=<MseLossBackward>) 10\n",
      "tensor(1.8379, device='cuda:0', grad_fn=<MseLossBackward>) 11\n",
      "tensor(1.7276, device='cuda:0', grad_fn=<MseLossBackward>) 12\n",
      "tensor(1.4037, device='cuda:0', grad_fn=<MseLossBackward>) 13\n",
      "tensor(1.3237, device='cuda:0', grad_fn=<MseLossBackward>) 14\n",
      "tensor(1.2920, device='cuda:0', grad_fn=<MseLossBackward>) 15\n",
      "tensor(1.0821, device='cuda:0', grad_fn=<MseLossBackward>) 16\n",
      "tensor(1.1650, device='cuda:0', grad_fn=<MseLossBackward>) 17\n",
      "tensor(0.9234, device='cuda:0', grad_fn=<MseLossBackward>) 18\n",
      "tensor(0.8429, device='cuda:0', grad_fn=<MseLossBackward>) 19\n",
      "tensor(0.8941, device='cuda:0', grad_fn=<MseLossBackward>) 20\n",
      "tensor(0.7719, device='cuda:0', grad_fn=<MseLossBackward>) 21\n",
      "tensor(0.7550, device='cuda:0', grad_fn=<MseLossBackward>) 22\n",
      "tensor(0.7663, device='cuda:0', grad_fn=<MseLossBackward>) 23\n",
      "tensor(0.6939, device='cuda:0', grad_fn=<MseLossBackward>) 24\n"
     ]
    }
   ],
   "source": [
    "#trainingloop\n",
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.0001\n",
    "batchsize=100\n",
    "\n",
    "batches=nsamples/batchsize\n",
    "\n",
    "epochs=25\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.Adagrad(net.parameters(), lr)\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    for j in range(int(batches)):\n",
    "        \n",
    "        #forward pass\n",
    "        out=net(Xtensor[j:j+batchsize,:]).type(dtype)\n",
    "\n",
    "        #compute loss\n",
    "        loss = customloss(out).type(dtype)\n",
    "\n",
    "\n",
    "        #backprop loss i.e. find dloss/dparam for each parameter and store.\n",
    "        loss.backward(retain_graph=True)\n",
    "        \n",
    "        #clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "    print(loss,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net(Xtensor[0:4,0:10])\n",
    "n=net(Xtensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[45.3694],\n",
       "        [43.6997],\n",
       "        [44.0106],\n",
       "        [44.6085],\n",
       "        [44.5296],\n",
       "        [44.6420],\n",
       "        [45.6386],\n",
       "        [43.8766],\n",
       "        [44.4748],\n",
       "        [44.1416],\n",
       "        [44.0472],\n",
       "        [44.7120],\n",
       "        [45.5437],\n",
       "        [44.5248],\n",
       "        [43.9113],\n",
       "        [44.2660],\n",
       "        [45.6405],\n",
       "        [44.4365],\n",
       "        [45.3489],\n",
       "        [45.0733]], device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(net(Xtensor[0:20,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 1)\n",
      "0    79.809982\n",
      "dtype: float32\n",
      "0    22.760593\n",
      "dtype: float32\n",
      "0    42.43692\n",
      "dtype: float32\n"
     ]
    }
   ],
   "source": [
    "result=pd.DataFrame(model(n).detach().cpu().numpy())\n",
    "print(result.shape)\n",
    "print(result.max())\n",
    "print(result.min())\n",
    "print(result.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([45.0036], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "9.443886756896973\n"
     ]
    }
   ],
   "source": [
    "p=0\n",
    "preddata=[]\n",
    "for i in range(len(n)):\n",
    "    if result.iloc[i,0]>45-0.01:\n",
    "        if result.iloc[i,0]<45+0.01:\n",
    "            p=p+1\n",
    "            preddata.append(n[i].detach().cpu().numpy())\n",
    "\n",
    "    \n",
    "print(p)\n",
    "preddata=np.array(preddata)\n",
    "preddata.shape\n",
    "\n",
    "for i in range(len(preddata)):\n",
    "    predtensori=torch.from_numpy(preddata[i]).type(dtype)\n",
    "    print(model(predtensori))\n",
    "    \n",
    "preddata=pd.DataFrame(preddata)\n",
    "preddata.to_csv('d45.csv')\n",
    "print((preddata.mean()).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
